{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple News Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../MarketWatch-Web-Crawler/latest.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "#### Download NLP Corpora\n",
    "two corpora: the stopwords corpus for removing stopwords and wordnet for lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/shiyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shiyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "lemma_english_stopwords = [wnl.lemmatize(x) for x in stopwords.words('english')]\n",
    "\n",
    "def preprocess(paragraphs):\n",
    "    message = ' '.join(paragraphs)\n",
    "    \n",
    "    # Lowercase the message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize wotwo corpora: the stopwords corpus for removing stopwords and wordnet for lemmatizing.\n",
    "\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(x) for x in tokens if len(x)>1]\n",
    "    tokens = [x for x in tokens if x not in lemma_english_stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if d['frontpage_summary']['label'] == 'flash headline':\n",
    "        d['token'] = preprocess(d['frontpage_summary']['headline'])\n",
    "    else:\n",
    "        d['token'] = preprocess(d['paragraphs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of tokenized article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['courtship',\n",
       " 'aurora',\n",
       " 'cannabis',\n",
       " 'inc',\n",
       " 'reliva',\n",
       " 'began',\n",
       " 'many',\n",
       " 'romance',\n",
       " 'gathering',\n",
       " 'industry',\n",
       " 'bigwig',\n",
       " 'banker',\n",
       " 'quite',\n",
       " 'love',\n",
       " 'first',\n",
       " 'sight',\n",
       " 'well',\n",
       " 'ahead',\n",
       " 'first',\n",
       " 'meeting']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[50]['token'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### why do I tokenize data while using VADER?\n",
    "\n",
    "Since punctuation, capitalization and use of intensifiers are more rarely used in serious news articles than social media posts, I still lowered and lemmatized the words, so try to only rely on the words sentiment from this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on News\n",
    "\n",
    "## VaderSentiment\n",
    "https://github.com/cjhutto/vaderSentiment#python-demo-and-code-examples\n",
    "\n",
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "\n",
    "Threshold values it used are:\n",
    "- positive sentiment: compound score >= 0.05\n",
    "- neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "- negative sentiment: compound score <= -0.05\n",
    "\n",
    "VADER judges sentiment by \n",
    "- Punctuation — I love pizza vs I love pizza!!\n",
    "- Capitalization — I’m hungry!! vs I’M HUNGRY!!\n",
    "- Degree modifiers (use of intensifiers)— I WANT TO EAT!! VS I REALLY WANT TO EAT!!\n",
    "- Conjunctions (shift in sentiment polarity, with later dictating polarity) — I love pizza, but I really hate Pizza Hut (bad review)\n",
    "- Preceding Tri-gram (identifying reverse polarity by examining the tri-gram before the lexical feature— Canadian Pizza is not really all that great.\n",
    "\n",
    "\n",
    "VADER is focused on social media and short texts while Financial News are almost the opposite. So I update the VADER lexicon with words+sentiments from the Loughran-McDonald Financial Sentiment Word Lists.\n",
    "\n",
    "## Loughran McDonald Sentiment Word Lists\n",
    "data source: https://sraf.nd.edu/textual-analysis/resources/\n",
    "\n",
    "Loughran McDonald Sentiment Word Lists categorizes words into the 7 sentiments. It's built using exclusively corporate disclosures.\n",
    "\n",
    "- Negative\n",
    "- Positive\n",
    "- Uncertainty\n",
    "- Litigious\n",
    "- StrongModal\n",
    "- WeakModal\n",
    "- Constraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Loughran McDonald Sentiment Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative :\t ['ABANDON' 'ABANDONED' 'ABANDONING' 'ABANDONMENT' 'ABANDONMENTS'] ...\n",
      "Positive :\t ['ABLE' 'ABUNDANCE' 'ABUNDANT' 'ACCLAIMED' 'ACCOMPLISH'] ...\n",
      "Uncertainty :\t ['ABEYANCE' 'ABEYANCES' 'ALMOST' 'ALTERATION' 'ALTERATIONS'] ...\n",
      "Litigious :\t ['ABOVEMENTIONED' 'ABROGATE' 'ABROGATED' 'ABROGATES' 'ABROGATING'] ...\n",
      "StrongModal :\t ['ALWAYS' 'BEST' 'CLEARLY' 'DEFINITELY' 'DEFINITIVELY'] ...\n",
      "WeakModal :\t ['ALMOST' 'APPARENTLY' 'APPEARED' 'APPEARING' 'APPEARS'] ...\n",
      "Constraining :\t ['ABIDE' 'ABIDING' 'BOUND' 'BOUNDED' 'COMMIT'] ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "sentiments = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 'StrongModal', 'WeakModal', 'Constraining']\n",
    "\n",
    "# read sentiment words\n",
    "path_LM = os.path.join('data', 'LoughranMcDonald_SentimentWordLists_2018.xlsx')\n",
    "dict_LM = {} # sentiment: [word list]\n",
    "for s in sentiments:\n",
    "    df_temp = pd.read_excel(path_LM, s, header=None)\n",
    "    dict_LM[s] = np.array(df_temp.iloc[:,0])\n",
    "    \n",
    "# print out sentiment+words\n",
    "for s, word_list in dict_LM.items():\n",
    "    print(s,':\\t', word_list[:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "new_words = {}\n",
    "for w in dict_LM['Negative']:\n",
    "    new_words[w.lower()] = -1\n",
    "for w in dict_LM['Positive']:\n",
    "    new_words[w.lower()] = 1\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sia.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d['sentiment_scores'] = sia.polarity_scores(' '.join(d['token']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out scores with results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline: It’s ‘no fun’ to be a small fund manager most of the time, Jan van Eck says — but right now is an exception\n",
      "\t {'neg': 0.087, 'neu': 0.738, 'pos': 0.175, 'compound': 0.9973}\n",
      "headline: Virgin Orbit’s first test launch postponed\n",
      "\t {'neg': 0.057, 'neu': 0.943, 'pos': 0.0, 'compound': -0.6249}\n",
      "headline: It’s ‘no fun’ to be a small fund manager most of the time, Jan van Eck says — but right now is an exception\n",
      "\t {'neg': 0.087, 'neu': 0.738, 'pos': 0.175, 'compound': 0.9973}\n",
      "headline: Is your city reopening after coronavirus lockdown? Scientists say avoid these places\n",
      "\t {'neg': 0.096, 'neu': 0.838, 'pos': 0.066, 'compound': -0.9638}\n",
      "headline: ‘I owe child support from my first marriage and did not receive a stimulus check. Does Trump not realize I have another family to take care of?’\n",
      "\t {'neg': 0.127, 'neu': 0.667, 'pos': 0.206, 'compound': 0.9935}\n",
      "headline: Shocking before-and-after photo shows what the coronavirus did to an otherwise healthy man in just 6 weeks\n",
      "\t {'neg': 0.153, 'neu': 0.706, 'pos': 0.141, 'compound': -0.8481}\n",
      "headline: Palo Alto Networks stock surges as earnings, outlook boosted by work-from-home\n",
      "\t {'neg': 0.027, 'neu': 0.778, 'pos': 0.195, 'compound': 0.9849}\n",
      "headline: CDC faces more coronavirus testing questions, this time about how many diagnostic tests are being conducted\n",
      "\t {'neg': 0.067, 'neu': 0.775, 'pos': 0.158, 'compound': 0.9874}\n",
      "headline: Today’s stock market looks like March 2009, before the longest bull run in history, says Morgan Stanley’s Mike Wilson\n",
      "\t {'neg': 0.163, 'neu': 0.659, 'pos': 0.179, 'compound': 0.8288}\n",
      "headline: Forget bonds — here are 5 safe tech stocks offering dividends and growth\n",
      "\t {'neg': 0.074, 'neu': 0.764, 'pos': 0.163, 'compound': 0.997}\n",
      "headline: ‘Project Birch’: Treasury draws up last resort bailout plan to rescue Britain’s largest companies\n",
      "\t {'neg': 0.122, 'neu': 0.703, 'pos': 0.174, 'compound': 0.9648}\n",
      "headline: German court says Volkswagen must buy diesel cars with faulty emissions testing\n",
      "\t {'neg': 0.124, 'neu': 0.731, 'pos': 0.144, 'compound': 0.5574}\n",
      "headline: Clorox and Netflix Shares Have Prospered in the Pandemic. But the Risk Is Rising.\n",
      "\t {'neg': 0.0, 'neu': 0.956, 'pos': 0.044, 'compound': 0.296}\n",
      "headline: Charlamagne Tha God tells Biden that black voters ‘saved your political life in the primaries’ and ‘have things they want from you’\n",
      "\t {'neg': 0.083, 'neu': 0.798, 'pos': 0.12, 'compound': 0.7134}\n",
      "headline: ‘I feel great to be out here’: Biden lays wreath at veterans memorial\n",
      "\t {'neg': 0.12, 'neu': 0.8, 'pos': 0.08, 'compound': -0.9705}\n",
      "headline: ‘It’s time for China to blink first,’ says Rep. Sherman, who leads the drive to delist China stocks\n",
      "\t {'neg': 0.125, 'neu': 0.708, 'pos': 0.167, 'compound': 0.9683}\n",
      "headline: It’s ‘no fun’ to be a small fund manager most of the time, Jan van Eck says — but right now is an exception\n",
      "\t {'neg': 0.087, 'neu': 0.738, 'pos': 0.175, 'compound': 0.9973}\n",
      "headline: Alibaba sees China retail volume growing near pre-pandemic levels but stock falls amid U.S.-China tensions\n",
      "\t {'neg': 0.146, 'neu': 0.722, 'pos': 0.132, 'compound': -0.2023}\n",
      "headline: The Great Outdoors Is Open for Business\n",
      "\t {'neg': 0.034, 'neu': 0.862, 'pos': 0.103, 'compound': 0.5574}\n",
      "headline: Virus, economy, Trump and cash hamper GOP’s bid for House\n",
      "\t {'neg': 0.15, 'neu': 0.712, 'pos': 0.139, 'compound': -0.4027}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for d in random.choices(data,k=20):\n",
    "    print(\"headline: \" + d['frontpage_summary']['headline'])\n",
    "    print(\"\\t\",d['sentiment_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For most of the articles, the sentiments result shows neutral. But the relative values of Negative and Positive scores do shed some light to the sentiment.\n",
    "\n",
    "#### Here are the Next Steps I could follow:\n",
    "1. parse articles for a longer period of time, and from more sources\n",
    "2. link the article sentiment analysis to referenced stock\n",
    "3. use stock sentiment to predict future return\n",
    "4. construct alhpa signals from the stock sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(py37_ml)",
   "language": "python",
   "name": "py37_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
